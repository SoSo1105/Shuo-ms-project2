{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Libraries\nimport os\nimport re\nimport csv\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, Embedding, Dropout, Activation, LSTM, Lambda\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers.pooling import GlobalAveragePooling1D\nimport keras.backend as K\nfrom math import pow, floor\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport tensorflow as tf\nfrom keras.models import load_model\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n        Only computes a batch-wise average of recall.\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n        Only computes a batch-wise average of precision.\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n\n# Data set path\nData_Dir = '../input/quora-question-pairs/'\n\n# Train_data & test_data\nTrain_Data_File = Data_Dir + 'train.csv'\nTest_Data_File = Data_Dir + 'test.csv'\n\n# Sequence length\nMax_Sequence_Length = 40\n\n# The max-number of words for segmentation\nMax_Num_Words = 100000\n\n# Dimensions of embedding\nEmbedding_Dim = 300\n\n# Validation set ratio\nValidation_Split_Ratio = 0.1\n\n# The parameters of lstm and FC layers\nNum_Lstm = 256\nNum_Dense = 125\nRate_Drop_Lstm = 0.15\nRate_Drop_Dense = 0.15\n\n\n# Avtivation function\nact_f = 'relu'\n\n# Data processing\n# To get the train data\ndf_train = pd.read_csv(Train_Data_File, encoding='utf-8')\ndf_train = df_train.fillna('empty')\ntrain_texts_1 = df_train.question1.tolist()\ntrain_texts_2 = df_train.question2.tolist()\ntrain_labels = df_train.train_labels.values\n\nprint('{} texts are found in train.csv'.format(len(train_texts_1)))\n\n# To get the test data\ndf_test = pd.read_csv(Test_Data_File, encoding='utf-8')\ndf_test = df_test.fillna('empty')\ntest_texts_1 = df_test.question1.tolist()\ntest_texts_2 = df_test.question2.tolist()\ntest_labels = df_test.train_labels.values\n\nprint('{} texts are found in test.csv'.format(len(test_texts_1)))\n\n# Encoding the setence and segmentation\ntokenizer = Tokenizer(num_words=Max_Num_Words)\ntokenizer.fit_on_texts(train_texts_1 + train_texts_2 + test_texts_1 + test_texts_2)\n\ntrain_sequences_1 = tokenizer.texts_to_sequences(train_texts_1)\ntrain_sequences_2 = tokenizer.texts_to_sequences(train_texts_2)\ntest_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\ntest_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n\nword_index = tokenizer.word_index\nprint('{} unique tokens are found'.format(len(word_index)))\n\n# Sentence padding for train_data\ntrain_data_1 = pad_sequences(train_sequences_1, maxlen=Max_Sequence_Length)\ntrain_data_2 = pad_sequences(train_sequences_2, maxlen=Max_Sequence_Length)\n\nprint('Shape of train data tensor:', train_data_1.shape)\nprint('Shape of train labels tensor:', train_labels.shape)\n\n# Sentence padding for test_data\ntest_data_1 = pad_sequences(test_sequences_1, maxlen=Max_Sequence_Length)\ntest_data_2 = pad_sequences(test_sequences_2, maxlen=Max_Sequence_Length)\n\nprint('Shape of test data tensor:', test_data_2.shape)\nprint('Shape of test ids tensor:', test_labels.shape)\n\n\n# Features extraction\nquestions = pd.concat([df_train[['question1', 'question2']], \\\n                       df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n\nq_dict = defaultdict(set)\nfor i in range(questions.shape[0]):\n    q_dict[questions.question1[i]].add(questions.question2[i])\n    q_dict[questions.question2[i]].add(questions.question1[i])\n\n# The length of qusetion sequence\ndef q1_freq(row):\n    return (len(q_dict[row['question1']]))\n\ndef q2_freq(row):\n    return (len(q_dict[row['question2']]))\n\n# Sequence similarity feature\ndef q1_q2_intersect(row):\n    return (len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n\n# Leaks feature\ndf_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect, axis=1, raw=True)\ndf_train['q1_freq'] = df_train.apply(q1_freq, axis=1, raw=True)\ndf_train['q2_freq'] = df_train.apply(q2_freq, axis=1, raw=True)\n\ndf_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect, axis=1, raw=True)\ndf_test['q1_freq'] = df_test.apply(q1_freq, axis=1, raw=True)\ndf_test['q2_freq'] = df_test.apply(q2_freq, axis=1, raw=True)\n\nleaks = df_train[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\ntest_leaks = df_test[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n\n# Data standardization\nss = StandardScaler()\nss.fit(np.vstack((leaks, test_leaks)))\nleaks = ss.transform(leaks)\ntest_leaks = ss.transform(test_leaks)\n\n\n# Word vectorization\n\n# The max number of words\nnum_words = min(Max_Num_Words, len(word_index)) + 1\n\ndef get_model():\n\n    # The embedding layer containing the word vectors\n\n    emb_layer = Embedding(\n        input_dim=num_words,\n        output_dim=Embedding_Dim,\n        input_length=Max_Sequence_Length,\n        trainable=False\n    )\n\n    # LSTM layer\n\n    lstm_layer = LSTM(Num_Lstm, dropout=Rate_Drop_Lstm, recurrent_dropout=Rate_Drop_Lstm)\n\n    # Define inputs\n    seq1 = Input(shape=(Max_Sequence_Length,), dtype='int32')\n    seq2 = Input(shape=(Max_Sequence_Length,), dtype='int32')\n\n    # Run inputs through embedding\n    emb1 = emb_layer(seq1)\n    emb2 = emb_layer(seq2)\n\n    # Run through LSTM layers\n    lstm_a = lstm_layer(emb1)\n    lstm_b = lstm_layer(emb2)\n\n    magic_input = Input(shape=(leaks.shape[1],))\n    # magic_dense = BatchNormalization()(magic_input)\n    magic_dense = Dense(int(Num_Dense / 2), activation=act_f)(magic_input)\n\n    diff = Lambda(lambda x: K.abs(x[0]-x[1]))([lstm_a, lstm_b])\n    mul = Lambda(lambda x: (x[0]*x[1]))([lstm_a, lstm_b])\n\n    merged = concatenate([lstm_a, lstm_b, magic_dense, diff, mul])\n    # merged = BatchNormalization()(merged)\n    merged = Dropout(Rate_Drop_Dense)(merged)\n\n    merged = Dense(Num_Dense, activation=act_f)(merged)\n    # merged = BatchNormalization()(merged)\n    merged = Dropout(Rate_Drop_Dense)(merged)\n\n    preds = Dense(1, activation='sigmoid')(merged)\n\n\n\n    # Train the model\n\n    model = Model(inputs=[seq1, seq2, magic_input], outputs=preds)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1])\n\n    # Set early stopping (large patience should be useful)\n    return model\n\n\nmodel = get_model()\n# Set early stopping (large patience should be useful)\nearly_stopping = EarlyStopping(monitor='val_loss', patience=20)\n\n# The model save path\nbst_model_path = 'model-params.h5'\nmodel_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n\n# Set the class_weight for directional training\nclass_weight = {0: 1.309033281, 1: 0.471544715}\n\n\nhist = model.fit([train_data_1, train_data_2, leaks], train_labels,\n                  validation_data=({\n                                      'input_1': test_data_1, \n                                      'input_2': test_data_2, \n                                      'input_3': test_leaks}, test_labels),\n                 # validation_split=0.1,\n                 epochs=50, batch_size=512, shuffle=True,\n                 class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n\n\n# model.save('t3.h5')\n\n# Export report\nreport = pd.DataFrame(hist.history)\nreport.to_excel('report.xlsx', index=False)\n\n\nreport[['loss', 'val_loss']].plot(marker='o')\nplt.xlabel('epochs')\nplt.ylabel('log loss')\n\n\nreport[['acc', 'val_acc']].plot(marker='o')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\n\nreport[['f1', 'val_f1']].plot(marker='o')\nplt.xlabel('epochs')\nplt.ylabel('f1 score')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}