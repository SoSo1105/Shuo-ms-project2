{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "efb64013-6a10-4277-9ee2-e4eb1f72a256",
    "_uuid": "60384730-0e36-42e3-bd31-935ce9a1ad4b"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, LSTM, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "import keras.backend as K\n",
    "from math import pow, floor\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "# Data set path\n",
    "Data_Dir = './'\n",
    "\n",
    "# Train_data & test_data\n",
    "Train_Data_File = Data_Dir + 'train.csv'\n",
    "Test_Data_File = Data_Dir + 'test.csv'\n",
    "\n",
    "# Sequence length\n",
    "Max_Sequence_Length = 40\n",
    "\n",
    "# The max-number of words for segmentation\n",
    "Max_Num_Words = 100000\n",
    "\n",
    "# Dimensions of embedding\n",
    "Embedding_Dim = 300\n",
    "\n",
    "# Validation set ratio\n",
    "Validation_Split_Ratio = 0.1\n",
    "\n",
    "# The parameters of lstm and FC layers\n",
    "Num_Lstm = 256\n",
    "Num_Dense = 125\n",
    "Rate_Drop_Lstm = 0.15\n",
    "Rate_Drop_Dense = 0.15\n",
    "\n",
    "\n",
    "# Avtivation function\n",
    "act_f = 'relu'\n",
    "\n",
    "# Data processing\n",
    "# To get the train data\n",
    "df_train = pd.read_csv(Train_Data_File, encoding='utf-8')\n",
    "df_train = df_train.fillna('empty')\n",
    "train_texts_1 = df_train.question1.tolist()\n",
    "train_texts_2 = df_train.question2.tolist()\n",
    "train_labels = df_train.train_labels.values\n",
    "\n",
    "print('{} texts are found in train.csv'.format(len(train_texts_1)))\n",
    "\n",
    "# To get the test data\n",
    "df_test = pd.read_csv(Test_Data_File, encoding='utf-8')\n",
    "df_test = df_test.fillna('empty')\n",
    "test_texts_1 = df_test.question1.tolist()\n",
    "test_texts_2 = df_test.question2.tolist()\n",
    "test_labels = df_test.train_labels.values\n",
    "\n",
    "print('{} texts are found in test.csv'.format(len(test_texts_1)))\n",
    "\n",
    "# Encoding the setence and segmentation\n",
    "tokenizer = Tokenizer(num_words=Max_Num_Words)\n",
    "tokenizer.fit_on_texts(train_texts_1 + train_texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train_texts_1)\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train_texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# Sentence padding for train_data\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=Max_Sequence_Length)\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=Max_Sequence_Length)\n",
    "\n",
    "print('Shape of train data tensor:', train_data_1.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# Sentence padding for test_data\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=Max_Sequence_Length)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=Max_Sequence_Length)\n",
    "\n",
    "print('Shape of test data tensor:', test_data_2.shape)\n",
    "print('Shape of test ids tensor:', test_labels.shape)\n",
    "\n",
    "\n",
    "# Features extraction\n",
    "questions = pd.concat([df_train[['question1', 'question2']], \\\n",
    "                       df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(questions.shape[0]):\n",
    "    q_dict[questions.question1[i]].add(questions.question2[i])\n",
    "    q_dict[questions.question2[i]].add(questions.question1[i])\n",
    "\n",
    "# The length of qusetion sequence\n",
    "def q1_freq(row):\n",
    "    return (len(q_dict[row['question1']]))\n",
    "\n",
    "def q2_freq(row):\n",
    "    return (len(q_dict[row['question2']]))\n",
    "\n",
    "# Sequence similarity feature\n",
    "def q1_q2_intersect(row):\n",
    "    return (len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "# Leaks feature\n",
    "df_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "df_train['q1_freq'] = df_train.apply(q1_freq, axis=1, raw=True)\n",
    "df_train['q2_freq'] = df_train.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "df_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "df_test['q1_freq'] = df_test.apply(q1_freq, axis=1, raw=True)\n",
    "df_test['q2_freq'] = df_test.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "leaks = df_train[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "test_leaks = df_test[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "\n",
    "# Data standardization\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leaks, test_leaks)))\n",
    "leaks = ss.transform(leaks)\n",
    "test_leaks = ss.transform(test_leaks)\n",
    "\n",
    "\n",
    "# Word vectorization\n",
    "\n",
    "# The max number of words\n",
    "num_words = min(Max_Num_Words, len(word_index)) + 1\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    # The embedding layer containing the word vectors\n",
    "\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=Embedding_Dim,\n",
    "        input_length=Max_Sequence_Length,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    # LSTM layer\n",
    "\n",
    "    lstm_layer = LSTM(Num_Lstm, dropout=Rate_Drop_Lstm, recurrent_dropout=Rate_Drop_Lstm)\n",
    "\n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(Max_Sequence_Length,), dtype='int32')\n",
    "    seq2 = Input(shape=(Max_Sequence_Length,), dtype='int32')\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "    emb2 = emb_layer(seq2)\n",
    "\n",
    "    # Run through LSTM layers\n",
    "    lstm_a = lstm_layer(emb1)\n",
    "    lstm_b = lstm_layer(emb2)\n",
    "\n",
    "    magic_input = Input(shape=(leaks.shape[1],))\n",
    "    # magic_dense = BatchNormalization()(magic_input)\n",
    "    magic_dense = Dense(int(Num_Dense / 2), activation=act_f)(magic_input)\n",
    "\n",
    "    diff = Lambda(lambda x: K.abs(x[0]-x[1]))([lstm_a, lstm_b])\n",
    "    mul = Lambda(lambda x: (x[0]*x[1]))([lstm_a, lstm_b])\n",
    "\n",
    "    merged = concatenate([lstm_a, lstm_b, magic_dense, diff, mul])\n",
    "    # merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(Rate_Drop_Dense)(merged)\n",
    "\n",
    "    merged = Dense(Num_Dense, activation=act_f)(merged)\n",
    "    # merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(Rate_Drop_Dense)(merged)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "\n",
    "    model = Model(inputs=[seq1, seq2, magic_input], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1])\n",
    "\n",
    "    # Set early stopping (large patience should be useful)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "# The model save path\n",
    "bst_model_path = 'model-params.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Set the class_weight for directional training\n",
    "class_weight = {0: 1.309033281, 1: 0.471544715}\n",
    "\n",
    "\n",
    "hist = model.fit([train_data_1, train_data_2, leaks], train_labels,\n",
    "                 # validation_data=({\n",
                      #                'input_1': test_data_1,\n",
                      #                'input_2': test_texts_2,\n",
                      #                'input_3': test_leaks}, test_labels),\n",
    "                 validation_split=0.1,\n",
    "                 epochs=50, batch_size=512, shuffle=True,\n",
    "                 class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "\n",
    "model.save('t3.h5')\n",
    "\n",
    "# Export report\n",
    "report = pd.DataFrame(hist.history)\n",
    "report.to_excel('report.xlsx', index=False)\n",
    "\n",
    "report[['loss', 'val_loss']].plot(marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('log loss')\n",
    "\n",
    "\n",
    "report[['acc', 'val_acc']].plot(marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "report[['f1', 'val_f1']].plot(marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('f1 score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
